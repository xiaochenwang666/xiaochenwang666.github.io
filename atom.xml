<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xiaochen</title>
  
  
  <link href="http://xiaochenwang666.github.io/xiaochen/atom.xml" rel="self"/>
  
  <link href="http://xiaochenwang666.github.io/xiaochen/"/>
  <updated>2024-12-16T10:06:15.177Z</updated>
  <id>http://xiaochenwang666.github.io/xiaochen/</id>
  
  <author>
    <name>xiaochen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>about</title>
    <link href="http://xiaochenwang666.github.io/xiaochen/about/"/>
    <id>http://xiaochenwang666.github.io/xiaochen/about/</id>
    <published>2024-12-10T14:12:52.000Z</published>
    <updated>2024-12-16T10:06:15.177Z</updated>
    
    <content type="html"><![CDATA[<p>Hi~ I am <strong>Xiaochen Wang(王晓晨)</strong>, a master student at PKU-NLP group, supervised by Prof. Zhifang Sui. Previously, I completed my bachelor’s degree at Beijing University of Posts and Telecommunications (BUPT) and awarded Beijing Outstanding Graduate in July 2022.</p><p><font color="red"> <strong>I am looking for a phd opportunity, please contact me if you are interested!</strong> </font></p><h3 id="Research-Interests"><a href="#Research-Interests" class="headerlink" title="Research Interests:"></a>Research Interests:</h3><p>My research interests lie in LLM, MLLM and machine learning. My research mainly focus on How to Enhance the Reasoning Capability of LLM efficiently?  My past work includes prompting, efficient post-training, model merging and evaluation. After the emergence of OpenAI-O1, I also began studying the field of RLHF due to its performance in the scaling laws of LLMs and its potential applications to embodied intelligence.</p><p>I am happy to discuss potential collaboration opportunities, feel free to reach out!</p><h3 id="Publications"><a href="#Publications" class="headerlink" title="Publications"></a>Publications</h3><p>Full list of papers can be found at <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=9hxxM1UAAAAJ">Google Scholar</a>.<br>* indicates equal contribution</p><ul><li><p><strong>Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization</strong></p><p> <em>Yiyang Du*, <strong>Xiaochen Wang</strong>*, Zhifang Sui, Yang Liu</em></p><p> [CVPR 2025 under review]</p></li><li><p><strong>SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine</strong> [<a href="https://arxiv.org/abs/2410.17021">Link</a>]</p><p><em><strong>Xiaochen Wang</strong>, Junqing He, Zhe Yang, Yiru Wang, Xiangdi Meng, Kunhao Pan, Zhifang Sui</em></p><p>[NAACL 2024 under review]</p></li><li><p><strong>Statistical Dataset Evaluation: Reliability, Difficulty, and<br>Validity</strong> <a href="https://arxiv.org/abs/2212.09272">[Link]</a></p><p><em>Chengwen Wang*, Qingxiu Dong*, <strong>Xiaochen Wang</strong>, Haitao Wang, Zhifang Sui</em></p><p>[Natural Language Engineering under minor revision]</p></li><li><p><strong>PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA<br>Optimization</strong> <a href="https://arxiv.org/abs/2402.16141">[Link]</a></p><p><em>Xiangdi Meng, Damai Dai, Weiyao Luo, Zhe Yang, Shaoxiang Wu, <strong>Xiaochen Wang</strong>, Peiyi Wang, Qingxiu Dong, Liang Chen, Zhifang Sui</em></p><p><em>arxiv</em></p></li><li><p><strong>Abnormal Event Detection via Hypergraph Contrastive Learning</strong>  <a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611977653.ch80">[Link]</a></p><p><em>Bo Yan, Cheng Yang, Chuan Shi, Jiawei Liu, <strong>Xiaochen Wang</strong></em></p><p><em>SDM 2023</em></p></li></ul><h3 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h3><p> <strong>AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization</strong></p><p><em>Yiyang Du*, <strong>Xiaochen Wang</strong>*,Chi Chen, Peng Li, Zhifang Sui, Yang Liu</em></p><ul><li><strong>Problems:</strong> Different VLLMs have variations in their visual models and connector components, which pose challenges for integrating VLLM models. Additionally, the hyperparameter selection for existing model fusion methods generally requires labeled data, imposing certain limitations.</li><li><strong>Solutions:</strong> We made the first attempt to do merging in heterogeneous MLLMs, including mapping, merging and searching to solve architectural differences and achieve best performance without labeled data. </li><li><strong>Results:</strong>  Extensive experiments show that our AdaMMS outperforms existing model merging methods on multiple vision-language benchmarks. Additionally, our merged model exceeds Qwen2-VL-7B in most tasks, achieving state-of-the-art performance at the 7B scale.</li></ul><img src="/images/adamms.png" width="500"><p><a href="https://arxiv.org/abs/2402.16141"><strong>PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization</strong></a></p><p><em>Xiangdi Meng, Damai Dai, Weiyao Luo, Zhe Yang, Shaoxiang Wu, <strong>Xiaochen Wang</strong>, Peiyi Wang, Qingxiu Dong, Liang Chen, Zhifang Sui</em></p><ul><li><p><strong>Problems:</strong> Rank adjustment during training is fixed and optimal rank value is uncertain. Besides, parameter scale for LoRA conflicts with memory usage.</p></li><li><p><strong>Solutions:</strong> We propose PeriodicLoRA (PLoRA), a method that periodically unloads the LoRA matrices back to the original backbone with a ratio. This process generates a higher rank update matrix by accumulating low rank matrices without extra memory usage.</p></li><li><p><strong>Results:</strong> Experimental findings demonstrate that PLoRA with low rank outperforms LoRA, improving training speed by 80%.</p></li></ul><img src="https://arxiv.org/html/2402.16141v1/x1.png" width="600"><p> <a href="https://arxiv.org/pdf/2410.17021"><strong>SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine</strong></a> </p><p><em><strong>Xiaochen Wang</strong>, Junqing He, Zhe Yang, Yiru Wang, Xiangdi Meng, Kunhao Pan, Zhifang Sui</em></p><ul><li><strong>Problems:</strong>   Large Language models (LLMs) with few-shot methods underperform in multi-hop reasoning because of hallucination, error propagation and limited context length. </li><li><strong>Solutions:</strong>  We design a zero-shot FSM framework based on Finite State Machine that explicitly supports the model in the phases of decomposition, retrieval, and verification. FSM tackles one task at a time, decides the next action based on the current results and states.</li><li><strong>Results:</strong> Our results have a more standardized format and outperform COT on challenging datasets like Musique.</li></ul><img src="https://arxiv.org/html/2410.17021v1/x1.png" width="600"><p>**Can Large Multimodal Models Uncover Deep Semantics Behind Comic Strip? **</p><p><em><strong>Xiaochen Wang</strong>, Heming Xia, Zhifang Sui</em></p><ul><li>**Problems:**Comic strips can be considered as important frames in video. We use them to evaluate the capability of video-LLM to understand the deep meanings of sequential multiple images.</li><li><strong>Solutions:</strong> We design three tasks: next frame prediction, Reorganize and deep meaning to evaluate.</li></ul><p><a href="https://arxiv.org/abs/2212.09272"><strong>Statistical Dataset Evaluation: Reliability, Difficulty, and Validit</strong></a> </p><p><em>Chengwen Wang*, Qingxiu Dong*, <strong>Xiaochen Wang</strong>, Haitao Wang, Zhifang Sui</em></p><ul><li><p><strong>Problems:</strong>  Existing datasets have exposed numerous issues, leading to biased models and unreliable evaluation, without human evaluation of dataset quality. </p></li><li><p><strong>Solutions:</strong>  We seek to understand the statistical properties of datasets and address three fundamental dimensions: reliability, difficulty, and validity. Taking the Named Entity Recognition datasets as a case study, we introduce $9$ statistical metrics.</p></li><li><p><strong>Results:</strong> We studied how the scores of datasets on statistical metrics impact model performance and advocate for assessing dataset quality or making targeted improvements to datasets before training or testing models.</p></li></ul><h3 id="Awards"><a href="#Awards" class="headerlink" title="Awards"></a>Awards</h3><p>Beijing Outstanding Graduate in BUPT (2022)<br>Merit Student, BUPT (2020)</p><h3 id="Services"><a href="#Services" class="headerlink" title="Services"></a>Services</h3><p><strong>Reviewer</strong></p><p>ARR</p><p><strong>Secretary, Youth League Branch</strong><br> <em>Undergraduate</em></p><p>Led the class to earn the First-Class Outstanding Class Award. And honored with the Excellent Student Leader title for exceptional leadership.</p><p><strong>Organization Committee Member</strong><br> <em>Graduate</em></p><p>Coordinated and managed class activities and support initiatives.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hi~ I am &lt;strong&gt;Xiaochen Wang(王晓晨)&lt;/strong&gt;, a master student at PKU-NLP group, supervised by Prof. Zhifang Sui. Previously, I completed</summary>
      
    
    
    
    
  </entry>
  
</feed>
